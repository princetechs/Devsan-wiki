

## 1. First, I wanted to fine-tune a small model using JupyterLab

### â””â”€â”€ But I realized I first needed to understand tokenization.

#### â””â”€â”€ That led me to the history of tokenization:
- n-gram (used in classic ML)
- BPE (Byte Pair Encoding)
- WordPiece / SentencePiece

#### â””â”€â”€ I learned:
- Tokenization breaks text into subwords
- Then maps subwords to IDs (`input_ids`)
- Example: "I love NLP" â†’ ['I', 'Ä love', 'Ä NLP'] â†’ [40, 1212, 20275]

---

## 2. Then, I returned to the idea of fine-tuning.

### â””â”€â”€ Step 1: Clean your data
- Normalize text (lowercase, remove special chars)
- Decide whether to apply further preprocessing

### â””â”€â”€ Step 2: Understand Corpus
- Corpus = all training text
- It is needed to **train or use a tokenizer**

### â””â”€â”€ Step 3: Learn about Vocabulary
- Vocabulary is built from the corpus
- Used to convert tokens â†’ token IDs

#### â””â”€â”€ Important Realization:
- Token ID is based on **merge order**, not appearance count.
- Example: `"ieve"` â†’ ID 103 (not because it appeared 103 times)

---

## 3. Once I have vocabulary:

### â””â”€â”€ I can tokenize (encode) text
- `tokenizer.encode("Hello world")` â†’ `[15496, 995]` (for GPT-2)

### â””â”€â”€ Then I can send the input IDs to the model
- With the fine-tuning prompt/response pairs

---

```
ğŸ”¹ Want to fine-tune an LLM?
    â†“
ğŸ”¸ Learn tokenization (text â†’ tokens â†’ IDs)
    â†“
ğŸ”¸ Discover BPE & subword tokenization (vs n-grams)
    â†“
ğŸ”¸ Learn corpus is needed to build vocabulary
    â†“
ğŸ”¸ Vocabulary gives token â†’ ID mapping
    â†“
ğŸ”¸ Encode cleaned text to input_ids
    â†“
ğŸ”¸ Feed token IDs with labels into model for fine-tuning
```



## **1. I wanted to fine-tune an LLM. But firstâ€¦ what is tokenization?**

  

> â“ _How do I send text to a model?_

  

â¬‡ï¸

  

### **ğŸ”¹ 1.1**Â 

### **Tokenization**

  

> Convert text â†’ subword tokens â†’ token IDs

> Example:

```
"I love NLP" â†’ ['I', 'Ä love', 'Ä NLP'] â†’ [40, 1212, 20275]
```

---

## **ğŸ“š**Â 

## **2. This made me curious: How did tokenization evolve?**

  

### **ğŸ”¸ 2.1**Â 

### **n-gram**

- Old method: â€œI love NLPâ€ â†’ [â€œIâ€, â€œloveâ€, â€œNLPâ€], [â€œI loveâ€, â€œlove NLPâ€]
    
- Used in ML (Naive Bayes, TF-IDF), **not** in LLMs
    

  

### **ğŸ”¸ 2.2**Â 

### **Subword Tokenization**

- BPE (Byte Pair Encoding)
    
- WordPiece, SentencePiece
    

  

> âœ… These split unknown/rare words into common parts:

> "unbelievable" â†’ ['un', 'believ', 'able']

---

## **ğŸ§°**Â 

## **3. Before training or fine-tuning, what do I need?**

  

### **ğŸ”¹ 3.1**Â 

### **Clean your text data**

- Remove extra symbols, normalize casing, punctuation
    
- Not required: n-grams (LLMs donâ€™t use this)
    

  

### **ğŸ”¹ 3.2**Â 

### **Prepare your corpus**

  

> A **corpus** is the entire body of text used to:

  

- Train the tokenizer
    
- Build the vocabulary
    

---

## **ğŸ§¾**Â 

## **4. How is the Vocabulary built?**

  

### **ğŸ”¸ 4.1 Tokenizer training on corpus:**

- BPE scans text for most frequent subword pairs
    
- Builds vocabulary like:
    

```
"I" â†’ 40  
"love" â†’ 1212  
"believ" â†’ 103  
```

  

  

> â— **Important**: "believ" â†’ 103 means it was the **103rd merge**,

> **not** that it appeared 103 times.

---

## **ğŸ”**Â 

## **5. Understanding Token IDs**

  

### **ğŸ”¹ 5.1 Token IDs are fixed after tokenizer training**

- Once "I" = 40, it stays 40 forever
    
- Token ID â‰  frequency
    
- Vocabulary and token IDs are **frozen** before model training
    

---

## **ğŸ§ **Â 

## **6. Encoding Text for Fine-Tuning**

  

> You now have:

> âœ… Cleaned Text

> âœ… Tokenizer

> âœ… Vocabulary (token â†’ ID)

  

### **ğŸ”¸ 6.1 Tokenize and encode your text**

```
tokenizer.encode("I love NLP") â†’ [40, 1212, 20275]
```

### **ğŸ”¸ 6.2 This encoded sequence is sent to the model**

  

Used in fine-tuning datasets like:

```
{"prompt": "Translate 'Hello' to Hindi.", "response": "à¤¨à¤®à¤¸à¥à¤¤à¥‡"}
```

---

## **ğŸ”**Â 

## **7. Fine-Tuning the LLM**

  

Once you have:

- input_ids
    
- prompt-response pairs
    

  

You fine-tune by:

- Sending batches of tokenized data
    
- Updating the modelâ€™s weights (not token IDs)
    
- Improving performance on your specific task/domain
    

---

## **âœ… Final Flow Recap â€” Mind Map in One Glance:**

```
ğŸ”¹ Want to fine-tune an LLM?
    â†“
ğŸ”¸ Learn tokenization (text â†’ tokens â†’ IDs)
    â†“
ğŸ”¸ Discover BPE & subword tokenization (vs n-grams)
    â†“
ğŸ”¸ Learn corpus is needed to build vocabulary
    â†“
ğŸ”¸ Vocabulary gives token â†’ ID mapping
    â†“
ğŸ”¸ Encode cleaned text to input_ids
    â†“
ğŸ”¸ Feed token IDs with labels into model for fine-tuning
```

---

